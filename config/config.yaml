# =============================================================================
# Multi-Agent System Configuration v3.2.1
# =============================================================================
# Supports script-first architecture with token-based context management
#
# Target: ARC GPU cluster
# Context: 32K tokens (OLLAMA_CONTEXT_LENGTH=32768 set in RUN script)
# GPU: V100S-32GB
#
# MODEL RESOLUTION CHAIN (v3.2.1):
# ---------------------------------
# Every component resolves its model via utils.model_config.resolve_model().
# This file is level 3 in the priority chain:
#
#   1. CLI --model flag           (highest — overrides everything)
#   2. OLLAMA_MODEL env var       (set by RUN scripts / sbatch --export)
#   3. ► THIS FILE: ollama.model  (project default)
#   4. FALLBACK_MODEL constant    (utils/model_config.py — last resort)
#
# To change the model:
#   Per-run:     sbatch --export=OLLAMA_MODEL=llama3.1:70b ...
#   Per-project: edit ollama.model below
#   System-wide: edit FALLBACK_MODEL in utils/model_config.py
#
# Same chain applies to ollama.base_url via resolve_base_url().
# =============================================================================

# =============================================================================
# Ollama Settings
# =============================================================================
ollama:
  # Project default model — level 3 in the resolution chain.
  # Overridden by CLI --model (level 1) or OLLAMA_MODEL env var (level 2).
  #
  # Selection criteria for V100S-32GB:
  #   - Model weights (Q4_K_M) must fit mostly on GPU (<30 GiB)
  #   - Must leave room for KV cache at full context length
  #   - qwen3-coder:32b  ≈ 20 GiB weights → fits with ~10 GiB for 32K KV cache
  #   - qwen3-coder-next ≈ 48 GiB → requires CPU offload, causes 500 errors
  model: "qwen3-coder:32b"

  # Ollama API endpoint — level 3 in the resolution chain.
  # Overridden by CLI --ollama-url (level 1) or OLLAMA_HOST env var (level 2).
  base_url: "http://127.0.0.1:11434"

  # Must match OLLAMA_CONTEXT_LENGTH in the RUN script.
  # V100S-32GB comfortably handles 32K context for qwen3-coder:32b.
  # Increase to 65536 if VRAM allows and you update the RUN script to match.
  model_context_length: 32768

# =============================================================================
# Context Management (v3.2 - Sized for 32K model context)
# =============================================================================
# Token budget must fit within model_context_length minus overhead for
# system prompt (~1K tokens) and response generation (~2-4K tokens).
#
# Budget breakdown for 32K context:
#   ~1K   system prompt + instructions
#   ~25K  accumulated agent context (history, tool outputs, task state)
#   ~3K   current turn input
#   ~3K   response generation
# =============================================================================
context:
  # Maximum tokens per SubAgent context window
  # Set to ~25K to leave headroom within 32K model context
  max_tokens_per_task: 25000

  # Maximum tokens for any single tool output (logs, file contents, etc.)
  # Larger outputs are automatically paginated
  # Sized to fit comfortably in 25K task budget with room for history
  max_tool_output_tokens: 12000

  # Minimum tokens needed to attempt another iteration
  # If remaining < this, task is marked for skip/escalate
  min_tokens_to_continue: 3000

  # Target size for history summaries when truncating
  summary_target_tokens: 1500

  # Token estimation: characters per token
  # qwen3 tokenizer is slightly more efficient than llama (~3.5-4 chars/token)
  # Using 4 as conservative estimate
  chars_per_token: 4

  # Enable automatic history summarization when context fills
  auto_summarize: true

  # Percentage of context to keep as recent history (rest gets summarized)
  recent_history_percent: 30

# =============================================================================
# Agent Settings
# =============================================================================
agents:
  # Legacy setting - kept for backward compatibility
  # Context exhaustion (not iteration count) determines retry limits
  max_retries: 12

  timeout_seconds: 300
  enable_dynamic_tools: true

  # Script-first settings
  script_first:
    enabled: true

    # Always generate scripts (vs interactive completion)
    always_generate_script: true

    # Script output directory (relative to project)
    scripts_dir: "scripts"

    # Add standard headers to generated scripts
    add_script_headers: true

    # Verify outputs exist after script execution
    verify_outputs: true

    # Maximum script generation attempts per task
    max_script_generations: 5

  # Supported languages for script generation
  supported_languages:
    - python
    - r
    - bash
    - perl

  # Language detection keywords
  language_detection:
    python:
      - "scanpy"
      - "pandas"
      - "numpy"
      - "anndata"
      - "popv"
      - "squidpy"
      - ".py"
      - "h5ad"
    r:
      - "seurat"
      - "bioconductor"
      - "SingleR"
      - ".R"
      - "Rscript"
    bash:
      - "bash"
      - "shell"
      - ".sh"

# =============================================================================
# Master Prompt Document
# Living document that tracks pipeline state
# =============================================================================
master_document:
  enabled: true

  # State persistence file (JSON)
  state_file: "reports/master_prompt_state.json"

  # Human-readable status (Markdown)
  status_file: "reports/pipeline_status.md"

  # Auto-save after each step completion/failure
  auto_save: true

  # Include in final report
  include_in_report: true

  # Track these fields per step
  track_fields:
    - script_path
    - output_files
    - conda_env_yaml
    - error_summary
    - attempts
    - context_tokens_used

# =============================================================================
# SLURM Configuration
# =============================================================================
slurm:
  enabled: true

  # Default cluster for CPU subtasks
  default_cluster: "arc_compute1"

  # Default cluster for GPU subtasks (auto-selected by package detection)
  default_gpu_cluster: "arc_gpu1v100"

  # Polling settings
  poll_interval: 30       # Seconds between job status checks
  max_poll_attempts: 8640  # 8640 × 30s = 3 days coverage

  # Job naming prefix
  job_prefix: "agi"

  # Script submission settings
  script_submission:
    # Generate sbatch scripts (vs direct srun)
    use_sbatch: true

    # Save sbatch scripts for debugging/resubmission
    save_sbatch_scripts: true

    # Directory for sbatch scripts
    sbatch_dir: "slurm/scripts"

    # Directory for job logs
    logs_dir: "slurm/logs"

    # Wait for job completion (vs fire-and-forget)
    wait_for_completion: true

    # Timeout for waiting (0 = use max_poll_attempts * poll_interval)
    wait_timeout_seconds: 0

# =============================================================================
# Cluster Configurations (Summary - full definitions in cluster_config.yaml)
# =============================================================================
# These are loaded from config/cluster_config.yaml at runtime.
# The entries here are for reference and fallback only.
# =============================================================================
clusters:
  # ARC CPU (default for subtasks)
  arc_compute1:
    name: "ARC Compute1"
    description: "Default CPU partition - 65 nodes, 3-day max"
    default_partition: "compute1"
    default_cpus: 20
    default_memory: "64G"
    default_time: "1-00:00:00"
    has_gpu: false

  # ARC GPU (for GPU subtasks)
  arc_gpu1v100:
    name: "ARC GPU 1×V100"
    description: "V100 GPU partition - 22 nodes, 3-day max"
    default_partition: "gpu1v100"
    default_cpus: 80
    default_time: "1-00:00:00"
    has_gpu: true
    gpu_type: "v100"

  # Zeus CPU (legacy fallback)
  zeus:
    name: "Zeus CPU"
    description: "Legacy high-memory CPU cluster"
    default_partition: "normal"
    default_cpus: 4
    default_memory: "16G"
    default_time: "04:00:00"
    has_gpu: false
    partitions:
      normal:
        max_time: "7-00:00:00"
        max_cpus: 192
        max_memory: "1000G"
        has_gpu: false

# =============================================================================
# Resource Profiles (Auto-select based on task keywords)
# =============================================================================
resource_profiles:
  default:
    cpus: 20
    memory: "64G"
    time: "1-00:00:00"
    gpus: 0

  lightweight:
    cpus: 4
    memory: "16G"
    time: "02:00:00"
    gpus: 0
    keywords:
      - "download"
      - "copy"
      - "move"
      - "convert"
      - "format"

  bioinformatics:
    cpus: 20
    memory: "64G"
    time: "1-00:00:00"
    gpus: 0
    keywords:
      - "scanpy"
      - "anndata"
      - "squidpy"
      - "popv"
      - "spatial"
      - "h5ad"
      - "seurat"
      - "bioconductor"

  heavy_compute:
    cpus: 40
    memory: "128G"
    time: "2-00:00:00"
    gpus: 0
    keywords:
      - "alignment"
      - "mapping"
      - "assembly"
      - "star"
      - "cellranger"
      - "salmon"

  gpu_ml:
    cpus: 80
    memory: null    # No memory spec for GPU nodes
    time: "1-00:00:00"
    gpus: 1
    gpu_type: "v100"
    partition: "gpu1v100"
    keywords:
      - "torch"
      - "tensorflow"
      - "scvi"
      - "train"
      - "gpu"
      - "cuda"
      - "deep learning"

# =============================================================================
# Failure Diagnosis (Pattern-based auto-recovery)
# =============================================================================
failure_diagnosis:
  patterns:
    module_not_found:
      regex: "ModuleNotFoundError: No module named '(.+)'"
      action: "add_package"
      description: "Missing Python package"

    file_not_found:
      regex: "FileNotFoundError: .*No such file or directory: '(.+)'"
      action: "check_path"
      description: "Missing input file"

    memory_error:
      regex: "(MemoryError|OutOfMemoryError|CUDA out of memory)"
      action: "increase_resources"
      description: "Insufficient memory"

    timeout:
      regex: "(TIMEOUT|DUE TO TIME LIMIT|exceeded.*time)"
      action: "increase_time"
      description: "Job exceeded time limit"

    conda_error:
      regex: "(CondaError|PackagesNotFoundError|ResolvePackageNotFound)"
      action: "fix_environment"
      description: "Conda environment issue"

    permission_error:
      regex: "PermissionError: .*"
      action: "check_permissions"
      description: "File permission issue"

    slurm_error:
      regex: "(FAILED|NODE_FAIL|PREEMPTED)"
      action: "retry_different_node"
      description: "SLURM node failure"

# =============================================================================
# Workflow Settings
# =============================================================================
workflow:
  enable_checkpointing: true
  checkpoint_frequency: "per_subtask"
  max_execution_time_minutes: 4320   # 3 days (matching GPU partition max)

  master_document:
    enabled: true

  subtask:
    generate_script: true
    submit_via_slurm: true

  parallel:
    enabled: true
    max_parallel_jobs: 10
