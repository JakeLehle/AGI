# =============================================================================
# Multi-Agent System Configuration v1.2.0
# =============================================================================
# Supports four-phase sub-agent lifecycle with integrated diagnostic agent
#
# Target: ARC GPU cluster
# Context: 32K tokens (OLLAMA_CONTEXT_LENGTH=32768 set in RUN script)
# GPU: V100S-32GB
#
# MODEL RESOLUTION CHAIN (v3.2.1):
# ---------------------------------
# Every component resolves its model via utils.model_config.resolve_model().
# This file is level 3 in the priority chain:
#
#   1. CLI --model flag           (highest — overrides everything)
#   2. OLLAMA_MODEL env var       (set by RUN scripts / sbatch --export)
#   3. ► THIS FILE: ollama.model  (project default)
#   4. FALLBACK_MODEL constant    (utils/model_config.py — last resort)
#
# To change the model:
#   Per-run:     sbatch --export=OLLAMA_MODEL=llama3.1:70b ...
#   Per-project: edit ollama.model below
#   System-wide: edit FALLBACK_MODEL in utils/model_config.py
#
# Same chain applies to ollama.base_url via resolve_base_url().
#
# CHANGELOG v1.2.0:
# ---------------------------------
# - Added diagnostic_agent section (Phase 4 diagnostic agent config)
# - Added disk_management section (proactive/reactive quota management)
# - Added script_generation section (Phase 1 two-pass validation)
# - Increased agents.max_retries 12 → 15
# - Added java to supported_languages
# - Expanded failure_diagnosis.patterns with new error types
# - Added diagnostic_memory to reflexion section
# =============================================================================

# =============================================================================
# Ollama Settings
# =============================================================================
ollama:
  # Project default model — level 3 in the resolution chain.
  # Overridden by CLI --model (level 1) or OLLAMA_MODEL env var (level 2).
  #
  # Selection criteria for V100S-32GB:
  #   - Model weights (Q4_K_M) must fit mostly on GPU (<30 GiB)
  #   - Must leave room for KV cache at full context length
  #   - qwen3-coder:32b  ≈ 20 GiB weights → fits with ~10 GiB for 32K KV cache
  #   - qwen3-coder-next ≈ 48 GiB → requires CPU offload, causes 500 errors
  model: "qwen3-coder:latest"

  # Ollama API endpoint — level 3 in the resolution chain.
  # Overridden by CLI --ollama-url (level 1) or OLLAMA_HOST env var (level 2).
  base_url: "http://127.0.0.1:11434"

  # Must match OLLAMA_CONTEXT_LENGTH in the RUN script.
  # V100S-32GB comfortably handles 32K context for qwen3-coder:32b.
  # Increase to 65536 if VRAM allows and you update the RUN script to match.
  model_context_length: 32768

# =============================================================================
# Context Management (v3.2 - Sized for 32K model context)
# =============================================================================
# Token budget must fit within model_context_length minus overhead for
# system prompt (~1K tokens) and response generation (~2-4K tokens).
#
# Budget breakdown for 32K context:
#   ~1K   system prompt + instructions
#   ~25K  accumulated agent context (history, tool outputs, task state)
#   ~3K   current turn input
#   ~3K   response generation
# =============================================================================
context:
  # Maximum tokens per SubAgent context window
  # Set to ~25K to leave headroom within 32K model context
  max_tokens_per_task: 25000

  # Maximum tokens for any single tool output (logs, file contents, etc.)
  # Larger outputs are automatically paginated
  # Sized to fit comfortably in 25K task budget with room for history
  max_tool_output_tokens: 12000

  # Minimum tokens needed to attempt another iteration
  # If remaining < this, task is marked for skip/escalate
  min_tokens_to_continue: 3000

  # Target size for history summaries when truncating
  summary_target_tokens: 1500

  # Token estimation: characters per token
  # qwen3 tokenizer is slightly more efficient than llama (~3.5-4 chars/token)
  # Using 4 as conservative estimate
  chars_per_token: 4

  # Enable automatic history summarization when context fills
  auto_summarize: true

  # Percentage of context to keep as recent history (rest gets summarized)
  recent_history_percent: 30

# =============================================================================
# Agent Settings
# =============================================================================
agents:
  # v1.2.0: Increased from 12 → 15 to match diagnostic agent retry budget.
  # Context exhaustion (not iteration count) determines actual retry limits,
  # but this caps the maximum number of phase-4 retry cycles per subtask.
  max_retries: 15

  timeout_seconds: 300
  enable_dynamic_tools: true

  # Script-first settings
  script_first:
    enabled: true

    # Always generate scripts (vs interactive completion)
    always_generate_script: true

    # Script output directory (relative to project)
    scripts_dir: "scripts"

    # Add standard headers to generated scripts
    add_script_headers: true

    # Verify outputs exist after script execution
    verify_outputs: true

    # Maximum script generation attempts per task
    max_script_generations: 5

  # Supported languages for script generation
  # v1.2.0: Added java — dispatch table in utils/dependency_parser.py
  supported_languages:
    - python
    - r
    - bash
    - perl
    - java

  # Language detection keywords
  language_detection:
    python:
      - "scanpy"
      - "pandas"
      - "numpy"
      - "anndata"
      - "popv"
      - "squidpy"
      - ".py"
      - "h5ad"
    r:
      - "seurat"
      - "bioconductor"
      - "SingleR"
      - ".R"
      - "Rscript"
    bash:
      - "bash"
      - "shell"
      - ".sh"
    java:
      - "java"
      - "gatk"
      - "picard"
      - ".jar"

# =============================================================================
# Script Generation — Phase 1 (v1.2.0)
# =============================================================================
# Two-pass validated script generation:
#   1a. LLM generates structured outline from subtask
#   1b. LLM validates outline matches subtask goals (max N attempts)
#   1c. LLM generates full implementation from validated outline
#   1d. LLM validates implementation covers outline (max N attempts)
#
# Each validation gate retries up to max_*_attempts before escalating.
# =============================================================================
script_generation:
  # Maximum attempts to produce a valid outline before escalating
  max_outline_attempts: 3

  # Maximum attempts to produce a valid implementation before escalating
  max_implementation_attempts: 3

  # Supported languages — mirrors agents.supported_languages but kept here
  # for explicit Phase 1 configuration. The dispatch table in
  # utils/dependency_parser.py defines execution commands per language.
  supported_languages:
    - python
    - r
    - bash
    - perl
    - java

# =============================================================================
# Diagnostic Agent — Phase 4 (v1.2.0)
# =============================================================================
# The diagnostic agent is invoked when a subtask execution fails. It has
# its own token budget (separate from the sub-agent's 25K) and can perform
# limited direct actions (package installs, diagnostic commands) while
# returning prescriptions for code/config changes.
#
# Memory: Uses DiagnosticMemory (memory/diagnostic_memory.py) with a
# separate Qdrant collection (agi_diagnostic_solutions) that is shared
# globally across all tasks and projects.
#
# Environment variables (set in RUN_AGI_PIPELINE_*.sh):
#   AGI_DIAGNOSTIC_TOKENS             Token budget per invocation
#   AGI_DIAGNOSTIC_MAX_RETRIES        Max investigation iterations
#   AGI_DIAGNOSTIC_MAX_COMMANDS       Max diagnostic bash commands
#   AGI_SOLUTION_MEMORY_THRESHOLD     Similarity threshold for known fixes
# =============================================================================
diagnostic_agent:
  # Master switch — set false to disable diagnostic agent and fall back
  # to the legacy _reflect_and_update() approach
  enabled: true

  # Token budget for each diagnostic agent invocation.
  # Each invocation gets a fresh context window of this size.
  # The sub-agent can invoke the diagnostic agent multiple times across
  # its 15 retry iterations — each gets a fresh budget.
  max_tokens_per_invocation: 25000

  # Maximum retry/investigation iterations per single error investigation.
  # The diagnostic agent may run diagnostic commands, install packages,
  # and iterate on its hypothesis up to this many times per invocation.
  max_retries_per_error: 15

  # Maximum number of diagnostic bash commands the agent can execute
  # per invocation (prevents runaway command loops)
  max_diagnostic_commands: 10

  # Similarity threshold for DiagnosticMemory lookups.
  # If a stored solution has similarity >= this AND success_count > 1,
  # the diagnostic agent applies it directly without investigation.
  solution_memory_threshold: 0.85

  # Whether to bootstrap DiagnosticMemory with known solutions on first run.
  # See memory/diagnostic_memory.py → BOOTSTRAP_SOLUTIONS for the seed data.
  bootstrap_on_first_run: true

  # Error types the diagnostic agent handles.
  # Errors not in this list are escalated immediately.
  handled_error_types:
    - missing_package
    - code_error
    - syntax_error
    - runtime_logic_error
    - data_structure_error
    - memory_error
    - gpu_error
    - disk_quota_error
    - binary_not_found
    - permission_error
    - file_not_found
    - network_error
    - sbatch_config_error
    - conda_error

  # Direct actions the diagnostic agent can take without returning a
  # prescription (low-risk, high-frequency fixes)
  direct_actions:
    - conda_install        # Install a package into existing env
    - pip_install          # pip install into existing env
    - conda_clean          # Clean conda cache for disk quota
    - run_diagnostic_cmd   # Execute diagnostic bash commands
    - run_diagnostic_script  # Execute small Python/R diagnostic scripts
    - update_env_yaml      # Add/remove packages from env YAML

  # Actions that require the sub-agent to apply (high-risk, needs full
  # task context)
  prescription_actions:
    - edit_code            # Modify analysis script
    - change_config        # Modify sbatch configuration
    - rebuild_env          # Full environment rebuild from updated YAML
    - escalate             # Give up — error is unrecoverable

# =============================================================================
# Disk Management (v1.2.0)
# =============================================================================
# Manages the limited home directory space (~100GB) on HPC clusters where
# conda environments can quickly consume available quota.
#
# Environment variables (set in RUN_AGI_PIPELINE_*.sh):
#   AGI_HOME_QUOTA_GB                 Total home quota
#   AGI_DISK_CLEANUP_THRESHOLD_GB     Proactive cleanup trigger
#   AGI_DISK_EMERGENCY_THRESHOLD_GB   Emergency cleanup trigger
# =============================================================================
disk_management:
  # Master switch
  enabled: true

  # Total home directory quota in GB
  home_quota_gb: 100

  # When free space drops below this, run proactive cleanup before builds.
  # This runs conda clean --all --yes and removes stale agi_* environments.
  proactive_cleanup_threshold_gb: 15

  # When free space drops below this, trigger emergency cleanup.
  # More aggressive: also cleans pip cache and removes all non-active envs.
  emergency_cleanup_threshold_gb: 5

  # Estimated size of a typical conda environment build in GB.
  # Used by ensure_space_for_build() to determine if there's enough room.
  estimated_env_size_gb: 5

  # Prefix for conda environments created by the AGI pipeline.
  # Used to identify stale environments for cleanup.
  stale_env_prefix: "agi_"

  # Whether to clean up conda environments after successful task completion.
  # The env YAML is always preserved for rebuilds — only the actual env is removed.
  cleanup_env_on_success: true

# =============================================================================
# Master Prompt Document
# Living document that tracks pipeline state
# =============================================================================
master_document:
  enabled: true

  # State persistence file (JSON)
  state_file: "reports/master_prompt_state.json"

  # Human-readable status (Markdown)
  status_file: "reports/pipeline_status.md"

  # Auto-save after each step completion/failure
  auto_save: true

  # Include in final report
  include_in_report: true

  # Track these fields per step
  track_fields:
    - script_path
    - output_files
    - conda_env_yaml
    - error_summary
    - attempts
    - context_tokens_used

# =============================================================================
# SLURM Configuration
# =============================================================================
slurm:
  enabled: true

  # Default cluster for CPU subtasks
  default_cluster: "arc_compute1"

  # Default cluster for GPU subtasks (auto-selected by package detection)
  default_gpu_cluster: "arc_gpu1v100"

  # Polling settings
  poll_interval: 30       # Seconds between job status checks
  max_poll_attempts: 8640  # 8640 × 30s = 3 days coverage

  # Job naming prefix
  job_prefix: "agi"

  # Script submission settings
  script_submission:
    # Generate sbatch scripts (vs direct srun)
    use_sbatch: true

    # Save sbatch scripts for debugging/resubmission
    save_sbatch_scripts: true

    # Directory for sbatch scripts
    sbatch_dir: "slurm/scripts"

    # Directory for job logs
    logs_dir: "slurm/logs"

    # Wait for job completion (vs fire-and-forget)
    wait_for_completion: true

    # Timeout for waiting (0 = use max_poll_attempts * poll_interval)
    wait_timeout_seconds: 0

# =============================================================================
# Cluster Configurations (Summary - full definitions in cluster_config.yaml)
# =============================================================================
# These are loaded from config/cluster_config.yaml at runtime.
# The entries here are for reference and fallback only.
# =============================================================================
clusters:
  # ARC CPU (default for subtasks)
  arc_compute1:
    name: "ARC Compute1"
    description: "Default CPU partition - 65 nodes, 3-day max"
    default_partition: "compute1"
    default_cpus: 20
    default_time: "1-00:00:00"
    has_gpu: false

  # ARC GPU (for GPU subtasks)
  arc_gpu1v100:
    name: "ARC GPU 1×V100"
    description: "V100 GPU partition - 22 nodes, 3-day max"
    default_partition: "gpu1v100"
    default_cpus: 80
    default_time: "1-00:00:00"
    has_gpu: true
    gpu_type: "v100"

  # Zeus CPU (legacy fallback)
  zeus:
    name: "Zeus CPU"
    description: "Legacy high-memory CPU cluster"
    default_partition: "normal"
    default_cpus: 4
    default_memory: "16G"
    default_time: "04:00:00"
    has_gpu: false
    partitions:
      normal:
        max_time: "7-00:00:00"
        max_cpus: 192
        max_memory: "1000G"
        has_gpu: false

# =============================================================================
# Resource Profiles (Auto-select based on task keywords)
# =============================================================================
resource_profiles:
  default:
    cpus: 20
    time: "1-00:00:00"
    gpus: 0

  lightweight:
    cpus: 4
    time: "02:00:00"
    gpus: 0
    keywords:
      - "download"
      - "copy"
      - "move"
      - "convert"
      - "format"

  bioinformatics:
    cpus: 20
    time: "1-00:00:00"
    gpus: 0
    keywords:
      - "scanpy"
      - "anndata"
      - "squidpy"
      - "popv"
      - "spatial"
      - "h5ad"
      - "seurat"
      - "bioconductor"

  heavy_compute:
    cpus: 40
    time: "2-00:00:00"
    gpus: 0
    keywords:
      - "alignment"
      - "mapping"
      - "assembly"
      - "star"
      - "cellranger"
      - "salmon"

  gpu_ml:
    cpus: 80
    memory: null    # No memory spec for GPU nodes
    time: "1-00:00:00"
    gpus: 1
    gpu_type: "v100"
    partition: "gpu1v100"
    keywords:
      - "torch"
      - "tensorflow"
      - "scvi"
      - "train"
      - "gpu"
      - "cuda"
      - "deep learning"

# =============================================================================
# Failure Diagnosis (Pattern-based auto-recovery)
# =============================================================================
# v1.2.0: Expanded with new error types that the diagnostic agent classifies.
# The regex patterns here are used for initial classification; the diagnostic
# agent performs deeper analysis within its investigation protocol.
# =============================================================================
failure_diagnosis:
  patterns:
    module_not_found:
      regex: "ModuleNotFoundError: No module named '(.+)'"
      action: "add_package"
      description: "Missing Python package"

    file_not_found:
      regex: "FileNotFoundError: .*No such file or directory: '(.+)'"
      action: "check_path"
      description: "Missing input file"

    memory_error:
      regex: "(MemoryError|OutOfMemoryError|CUDA out of memory)"
      action: "increase_resources"
      description: "Insufficient memory"

    timeout:
      regex: "(TIMEOUT|DUE TO TIME LIMIT|exceeded.*time)"
      action: "increase_time"
      description: "Job exceeded time limit"

    conda_error:
      regex: "(CondaError|PackagesNotFoundError|ResolvePackageNotFound)"
      action: "fix_environment"
      description: "Conda environment issue"

    permission_error:
      regex: "PermissionError: .*"
      action: "check_permissions"
      description: "File permission issue"

    slurm_error:
      regex: "(FAILED|NODE_FAIL|PREEMPTED)"
      action: "retry_different_node"
      description: "SLURM node failure"

    # --- v1.2.0: New patterns for diagnostic agent classification -----------

    data_structure_error:
      regex: "(KeyError|IndexError|ValueError.*shape|AttributeError.*'NoneType'|'DataFrame' object has no attribute)"
      action: "diagnose_data"
      description: "Data structure mismatch or unexpected format"

    binary_not_found:
      regex: "(command not found|No such file or directory.*bin/|which: no .+ in)"
      action: "install_binary"
      description: "Missing system binary or CLI tool"

    disk_quota_error:
      regex: "(No space left on device|Disk quota exceeded|OSError.*ENOSPC)"
      action: "cleanup_disk"
      description: "Disk full or quota exceeded"

    network_error:
      regex: "(ConnectionError|URLError|TimeoutError.*connect|SSLError|requests.exceptions)"
      action: "retry_network"
      description: "Network connectivity issue"

    runtime_logic_error:
      regex: "(AssertionError|ZeroDivisionError|RecursionError|StopIteration)"
      action: "diagnose_code"
      description: "Runtime logic error in analysis script"

    sbatch_config_error:
      regex: "(sbatch: error|Invalid partition|invalid account|Batch job submission failed)"
      action: "fix_sbatch"
      description: "SLURM sbatch configuration error"

    r_error:
      regex: "(Error in .+ : |Execution halted|there is no package called)"
      action: "diagnose_r"
      description: "R script execution error"

# =============================================================================
# Workflow Settings
# =============================================================================
workflow:
  enable_checkpointing: true
  checkpoint_frequency: "per_subtask"
  max_execution_time_minutes: 4320   # 3 days (matching GPU partition max)

  master_document:
    enabled: true

  subtask:
    generate_script: true
    submit_via_slurm: true

  parallel:
    enabled: true
    max_parallel_jobs: 10

# =============================================================================
# Reflexion Memory
# =============================================================================
# Per-task memory for preventing retry loops.
# Config file: config/mem0_config.yaml
# Backend: Mem0 + embedded Qdrant (collection: agi_reflexion_memory)
# =============================================================================
reflexion:
  enabled: true

  # Similarity threshold — approaches above this are considered duplicates
  similarity_threshold: 0.85

  # v1.2.0: Diagnostic solution memory (global, cross-task knowledge base)
  # Backend: Mem0 + embedded Qdrant (collection: agi_diagnostic_solutions)
  # See memory/diagnostic_memory.py
  diagnostic_memory:
    enabled: true

    # Separate Qdrant collection for diagnostic solutions
    collection_name: "agi_diagnostic_solutions"

    # Similarity threshold for confident solution match
    # When a stored solution exceeds this AND success_count > 1,
    # the diagnostic agent applies it without investigation
    similarity_threshold: 0.85

    # Auto-prune solutions unused for this many days
    # Solutions with success_count >= 3 are kept regardless
    prune_stale_days: 90

    # Bootstrap with known solutions on first initialization
    bootstrap_on_first_run: true
