# =============================================================================
# AGI Pipeline v1.2.0 - Cluster Configuration
# =============================================================================
# 
# PRIMARY TARGET: ARC HPC Cluster (GPU + CPU)
# FALLBACK: Zeus CPU Cluster (preserved for backward compatibility)
#
# Architecture:
#   - Master pipeline runs on ARC GPU node (Ollama + qwen3-coder:latest)
#   - Subtask scripts routed to GPU or CPU partitions based on task needs
#   - AGI_CLUSTER sets DEFAULT subtask target (compute1 for CPU tasks)
#   - AGI_GPU_CLUSTER sets GPU subtask target (gpu1v100 for GPU tasks)
#
# IMPORTANT - GPU NODE RULES:
#   - Do NOT specify --mem on GPU partitions (causes allocation failures)
#   - Use --gres=gpu:N format (not --gpus N)
#   - Standard GPU request: --gres=gpu:1 -N 1 -n 1 -c 80
#
# CHANGELOG v1.2.0:
#   - Added language_dispatch section for Phase 3 multi-language sbatch gen
#
# =============================================================================

default_cluster: arc_compute1
default_gpu_cluster: arc_gpu1v100

# =============================================================================
# ARC GPU PARTITIONS (for subtasks requiring GPU)
# =============================================================================
# NOTE: No memory directive on any GPU partition - let SLURM use defaults.
# Specifying memory on GPU nodes causes allocation failures.
# =============================================================================

clusters:

  # ---------------------------------------------------------------------------
  # ARC GPU 1×V100 - Primary GPU partition (22 nodes, best availability)
  # ---------------------------------------------------------------------------
  arc_gpu1v100:
    name: "ARC GPU 1×V100"
    description: "Single V100 GPU nodes - best availability for GPU tasks"
    
    slurm:
      partition: "gpu1v100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      # NO memory - GPU nodes must use defaults
      time: "1-00:00:00"
    
    limits:
      max_cpus: 80
      max_time: "3-00:01:00"
      nodes_total: 22
    
    gpu:
      available: true
      default_count: 1
      max_count: 1
      type: "v100"
      directive_format: "--gres=gpu:{count}"

  # ---------------------------------------------------------------------------
  # ARC GPU 2×V100 (9 nodes)
  # ---------------------------------------------------------------------------
  arc_gpu2v100:
    name: "ARC GPU 2×V100"
    description: "Dual V100 GPU nodes - for larger GPU workloads"
    
    slurm:
      partition: "gpu2v100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      time: "1-00:00:00"
    
    limits:
      max_cpus: 80
      max_time: "3-00:00:00"
      nodes_total: 9
    
    gpu:
      available: true
      default_count: 1
      max_count: 2
      type: "v100"
      directive_format: "--gres=gpu:{count}"

  # ---------------------------------------------------------------------------
  # ARC GPU 4×V100 (2 nodes)
  # ---------------------------------------------------------------------------
  arc_gpu4v100:
    name: "ARC GPU 4×V100"
    description: "Quad V100 GPU nodes - for heavy multi-GPU workloads"
    
    slurm:
      partition: "gpu4v100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      time: "1-00:00:00"
    
    limits:
      max_cpus: 80
      max_time: "3-00:00:00"
      nodes_total: 2
    
    gpu:
      available: true
      default_count: 1
      max_count: 4
      type: "v100"
      directive_format: "--gres=gpu:{count}"

  # ---------------------------------------------------------------------------
  # ARC GPU A100 (2 nodes)
  # ---------------------------------------------------------------------------
  arc_gpu1a100:
    name: "ARC GPU A100"
    description: "A100 GPU nodes - highest performance single-GPU"
    
    slurm:
      partition: "gpu1a100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      time: "1-00:00:00"
    
    limits:
      max_cpus: 80
      max_time: "3-00:00:00"
      nodes_total: 2
    
    gpu:
      available: true
      default_count: 1
      max_count: 1
      type: "a100"
      directive_format: "--gres=gpu:{count}"

  # ---------------------------------------------------------------------------
  # ARC DGX A100 (3 nodes, infinite time limit)
  # ---------------------------------------------------------------------------
  arc_dgxa100:
    name: "ARC DGX A100"
    description: "DGX A100 system - multi-GPU, no time limit"
    
    slurm:
      partition: "dgxa100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      time: "3-00:00:00"
    
    limits:
      max_cpus: 128
      max_time: "infinite"
      nodes_total: 3
    
    gpu:
      available: true
      default_count: 1
      max_count: 8
      type: "a100"
      directive_format: "--gres=gpu:{count}"

  # ===========================================================================
  # ARC CPU PARTITIONS (for subtasks NOT requiring GPU)
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # ARC Compute1 - Default CPU partition (65 nodes, 3-day max)
  # ---------------------------------------------------------------------------
  arc_compute1:
    name: "ARC Compute1"
    description: "Default CPU partition - best availability, 3-day max"
    
    slurm:
      partition: "compute1"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 20
      time: "1-00:00:00"
    
    limits:
      max_cpus: 48
      max_time: "3-00:01:00"
      nodes_total: 65
    
    gpu:
      available: false

  # ---------------------------------------------------------------------------
  # ARC Compute2 - Extended time CPU partition (27 nodes, 10-day max)
  # ---------------------------------------------------------------------------
  arc_compute2:
    name: "ARC Compute2"
    description: "Extended time CPU partition - 10-day max for long jobs"
    
    slurm:
      partition: "compute2"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 20
      time: "1-00:00:00"
    
    limits:
      max_cpus: 48
      max_time: "10-00:01:00"
      nodes_total: 27
    
    gpu:
      available: false

  # ---------------------------------------------------------------------------
  # ARC Compute3 - Additional CPU partition (6 nodes, 3-day max)
  # ---------------------------------------------------------------------------
  arc_compute3:
    name: "ARC Compute3"
    description: "Additional CPU partition - 3-day max"
    
    slurm:
      partition: "compute3"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 20
      time: "1-00:00:00"
    
    limits:
      max_cpus: 48
      max_time: "3-00:01:00"
      nodes_total: 6
    
    gpu:
      available: false

  # ===========================================================================
  # LEGACY: Zeus CPU Cluster (preserved for backward compatibility)
  # ===========================================================================

  zeus_cpu:
    name: "Zeus CPU Cluster"
    description: "Legacy Zeus cluster - CPU only, high memory nodes"
    
    slurm:
      partition: "normal"
      account: "jlehle"
      nodes: 1
      ntasks: 1
      cpus_per_task: 40
      memory: "64G"
      time: "08:00:00"
    
    limits:
      max_cpus: 192
      max_memory: "900G"
      max_time: "7-00:00:00"
      nodes_total: 10
    
    gpu:
      available: false

# =============================================================================
# GPU PACKAGE DETECTION
# =============================================================================
# Packages that indicate a subtask needs GPU resources.
# Used by sub-agent to route tasks to GPU vs CPU partitions.
# =============================================================================

gpu_packages:
  - torch
  - pytorch
  - tensorflow
  - keras
  - jax
  - rapids
  - cuml
  - cudf
  - cugraph
  - scvi-tools
  - scvi
  - scvelo
  - cellbender
  - flash-attn
  - xformers
  - bitsandbytes
  - accelerate
  - deepspeed
  - triton

# =============================================================================
# LANGUAGE DISPATCH (v1.2.0 - Phase 3 Multi-Language Sbatch Generation)
# =============================================================================
# Defines how to execute scripts in each supported language within an sbatch.
# Used by Phase 3 (sbatch generation) to produce the correct execution command,
# shebang line, and any pre-execution steps (compilation, permissions).
#
# The sub-agent's _generate_sbatch_for_language() method reads this table.
# The dispatch table in utils/dependency_parser.py mirrors this data for use
# outside of sbatch generation (diagnostic agent, language detection).
#
# Fields:
#   command       — Execution command template. {script} is replaced with the
#                   script path relative to project root.
#   shebang       — Shebang line for the generated sbatch wrapper (not the
#                   analysis script itself — the sbatch already has #!/bin/bash).
#                   This is informational for the sub-agent's script generation.
#   comment_prefix — Comment syntax for the language (used when adding headers).
#   permissions   — File permissions to set before execution (null = none needed).
#   compile_first — Compilation command to run before execution (null = none).
#                   {script} is replaced with the script path.
#   conda_base    — Base conda package required for this language runtime.
#                   Used by Phase 2 dependency extraction to ensure the runtime
#                   is always included in the environment YAML.
#   env_check_cmd — Command to verify a package is installed in the conda env.
#                   {package} is replaced with the package name.
# =============================================================================

language_dispatch:
  python:
    command: "python {script}"
    shebang: "#!/usr/bin/env python3"
    comment_prefix: "#"
    permissions: null
    compile_first: null
    conda_base: "python"
    env_check_cmd: 'python -c "import {package}"'

  r:
    command: "Rscript {script}"
    shebang: "#!/usr/bin/env Rscript"
    comment_prefix: "#"
    permissions: null
    compile_first: null
    conda_base: "r-base"
    env_check_cmd: 'Rscript -e "library({package})"'

  bash:
    command: "bash {script}"
    shebang: "#!/bin/bash"
    comment_prefix: "#"
    permissions: "+x"
    compile_first: null
    conda_base: null
    env_check_cmd: "which {package}"

  perl:
    command: "perl {script}"
    shebang: "#!/usr/bin/env perl"
    comment_prefix: "#"
    permissions: null
    compile_first: null
    conda_base: "perl"
    env_check_cmd: 'perl -e "use {package}"'

  java:
    command: "java -cp . {script}"
    shebang: null
    comment_prefix: "//"
    permissions: null
    compile_first: "javac {script}"
    conda_base: "openjdk"
    env_check_cmd: null
