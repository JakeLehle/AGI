# =============================================================================
# AGI Pipeline v3.2 - Cluster Configuration
# =============================================================================
# 
# PRIMARY TARGET: ARC HPC Cluster (GPU + CPU)
# FALLBACK: Zeus CPU Cluster (preserved for backward compatibility)
#
# Architecture:
#   - Master pipeline runs on ARC GPU node (Ollama + qwen3-coder-next)
#   - Subtask scripts routed to GPU or CPU partitions based on task needs
#   - AGI_CLUSTER sets DEFAULT subtask target (compute1 for CPU tasks)
#   - AGI_GPU_CLUSTER sets GPU subtask target (gpu1v100 for GPU tasks)
#
# IMPORTANT - GPU NODE RULES:
#   - Do NOT specify --mem on GPU partitions (causes allocation failures)
#   - Use --gres=gpu:N format (not --gpus N)
#   - Standard GPU request: --gres=gpu:1 -N 1 -n 1 -c 80
#
# =============================================================================

default_cluster: arc_compute1
default_gpu_cluster: arc_gpu1v100

# =============================================================================
# ARC GPU PARTITIONS (for subtasks requiring GPU)
# =============================================================================
# NOTE: No memory directive on any GPU partition - let SLURM use defaults.
# Specifying memory on GPU nodes causes allocation failures.
# =============================================================================

clusters:

  # ---------------------------------------------------------------------------
  # ARC GPU 1×V100 - Primary GPU partition (22 nodes, best availability)
  # ---------------------------------------------------------------------------
  arc_gpu1v100:
    name: "ARC GPU 1×V100"
    description: "Single V100 GPU nodes - best availability for GPU tasks"
    
    slurm:
      partition: "gpu1v100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      # NO memory - GPU nodes must use defaults
      time: "1-00:00:00"
    
    limits:
      max_cpus: 80
      max_time: "3-00:01:00"
      nodes_total: 22
    
    gpu:
      available: true
      default_count: 1
      max_count: 1
      type: "v100"
      directive_format: "--gres=gpu:{count}"

  # ---------------------------------------------------------------------------
  # ARC GPU 2×V100 (9 nodes)
  # ---------------------------------------------------------------------------
  arc_gpu2v100:
    name: "ARC GPU 2×V100"
    description: "Dual V100 GPU nodes - for larger GPU workloads"
    
    slurm:
      partition: "gpu2v100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      time: "1-00:00:00"
    
    limits:
      max_cpus: 80
      max_time: "3-00:00:00"
      nodes_total: 9
    
    gpu:
      available: true
      default_count: 1
      max_count: 2
      type: "v100"
      directive_format: "--gres=gpu:{count}"

  # ---------------------------------------------------------------------------
  # ARC GPU 4×V100 (2 nodes)
  # ---------------------------------------------------------------------------
  arc_gpu4v100:
    name: "ARC GPU 4×V100"
    description: "Quad V100 GPU nodes - for heavy multi-GPU workloads"
    
    slurm:
      partition: "gpu4v100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      time: "1-00:00:00"
    
    limits:
      max_cpus: 80
      max_time: "3-00:00:00"
      nodes_total: 2
    
    gpu:
      available: true
      default_count: 1
      max_count: 4
      type: "v100"
      directive_format: "--gres=gpu:{count}"

  # ---------------------------------------------------------------------------
  # ARC GPU A100 (2 nodes)
  # ---------------------------------------------------------------------------
  arc_gpu1a100:
    name: "ARC GPU A100"
    description: "A100 GPU nodes - highest performance single-GPU"
    
    slurm:
      partition: "gpu1a100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      time: "1-00:00:00"
    
    limits:
      max_cpus: 80
      max_time: "3-00:00:00"
      nodes_total: 2
    
    gpu:
      available: true
      default_count: 1
      max_count: 1
      type: "a100"
      directive_format: "--gres=gpu:{count}"

  # ---------------------------------------------------------------------------
  # ARC DGX A100 (3 nodes, infinite time limit)
  # ---------------------------------------------------------------------------
  arc_dgxa100:
    name: "ARC DGX A100"
    description: "DGX A100 system - multi-GPU, no time limit"
    
    slurm:
      partition: "dgxa100"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 80
      time: "3-00:00:00"
    
    limits:
      max_cpus: 128
      max_time: "infinite"
      nodes_total: 3
    
    gpu:
      available: true
      default_count: 1
      max_count: 8
      type: "a100"
      directive_format: "--gres=gpu:{count}"

  # ===========================================================================
  # ARC CPU PARTITIONS (for subtasks NOT requiring GPU)
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # ARC Compute1 - Default CPU partition (65 nodes, 3-day max)
  # ---------------------------------------------------------------------------
  arc_compute1:
    name: "ARC Compute1"
    description: "Default CPU partition - best availability, 3-day max"
    
    slurm:
      partition: "compute1"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 20
      memory: "64G"
      time: "1-00:00:00"
    
    limits:
      max_cpus: 48
      max_time: "3-00:01:00"
      nodes_total: 65
    
    gpu:
      available: false

  # ---------------------------------------------------------------------------
  # ARC Compute2 - Extended time CPU partition (27 nodes, 10-day max)
  # ---------------------------------------------------------------------------
  arc_compute2:
    name: "ARC Compute2"
    description: "Extended time CPU partition - 10-day max for long jobs"
    
    slurm:
      partition: "compute2"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 20
      memory: "64G"
      time: "1-00:00:00"
    
    limits:
      max_cpus: 48
      max_time: "10-00:01:00"
      nodes_total: 27
    
    gpu:
      available: false

  # ---------------------------------------------------------------------------
  # ARC Compute3 - Additional CPU partition (6 nodes, 3-day max)
  # ---------------------------------------------------------------------------
  arc_compute3:
    name: "ARC Compute3"
    description: "Additional CPU partition - 3-day max"
    
    slurm:
      partition: "compute3"
      account: "sdz852"
      nodes: 1
      ntasks: 1
      cpus_per_task: 20
      memory: "64G"
      time: "1-00:00:00"
    
    limits:
      max_cpus: 48
      max_time: "3-00:01:00"
      nodes_total: 6
    
    gpu:
      available: false

  # ===========================================================================
  # LEGACY: Zeus CPU Cluster (preserved for backward compatibility)
  # ===========================================================================

  zeus_cpu:
    name: "Zeus CPU Cluster"
    description: "Legacy Zeus cluster - CPU only, high memory nodes"
    
    slurm:
      partition: "normal"
      account: "jlehle"
      nodes: 1
      ntasks: 1
      cpus_per_task: 40
      memory: "64G"
      time: "08:00:00"
    
    limits:
      max_cpus: 192
      max_memory: "900G"
      max_time: "7-00:00:00"
      nodes_total: 10
    
    gpu:
      available: false

# =============================================================================
# GPU PACKAGE DETECTION
# =============================================================================
# Packages that indicate a subtask needs GPU resources.
# Used by sub-agent to route tasks to GPU vs CPU partitions.
# =============================================================================

gpu_packages:
  - torch
  - pytorch
  - tensorflow
  - keras
  - jax
  - rapids
  - cuml
  - cudf
  - cugraph
  - scvi-tools
  - scvi
  - scvelo
  - cellbender
  - flash-attn
  - xformers
  - bitsandbytes
  - accelerate
  - deepspeed
  - triton
